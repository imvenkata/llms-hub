{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf3tVGJCZihr"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/prompting/prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<!--- @wandbcode{prompt-engineering-course} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaVokP82Zihs"
      },
      "source": [
        "# Prompt Engineering with Weights & Biases - [Anish Shah](https://www.linkedin.com/in/anish-shah/)\n",
        "This is a companion notebook to the Weights & Biases [Prompt Engineering course](https://www.wandb.courses/courses/prompting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bkukrJ2UjfZt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install set-env-colab-kaggle-dotenv -q\n",
        "!pip install weave -U -q\n",
        "!pip install litellm -U -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q49iLEnJZiht",
        "outputId": "b9cfd5f3-f687-4e36-c0d3-6b93b60d6466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'edu'...\n",
            "remote: Enumerating objects: 4193, done.\u001b[K\n",
            "remote: Counting objects: 100% (1095/1095), done.\u001b[K\n",
            "remote: Compressing objects: 100% (367/367), done.\u001b[K\n",
            "remote: Total 4193 (delta 905), reused 753 (delta 725), pack-reused 3098 (from 1)\u001b[K\n",
            "Receiving objects: 100% (4193/4193), 27.72 MiB | 20.90 MiB/s, done.\n",
            "Resolving deltas: 100% (2300/2300), done.\n",
            "/content/edu/prompting\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    !git clone https://github.com/wandb/edu.git\n",
        "    %cd edu/prompting\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQexzODlZiht"
      },
      "source": [
        "To pass your environment keys the recommended approach is to use secrets, especially if using Google Colab.\n",
        "\n",
        "### How to add your key to Colab Secrets\n",
        "\n",
        "Add your API key to the Colab Secrets manager to securely store it.\n",
        "\n",
        "1. Open your Google Colab notebook and click on the 🔑 **Secrets** tab in the left panel.\n",
        "   \n",
        "   <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
        "\n",
        "2. Create a new secret with a name from below.\n",
        "3. Copy/paste your API key into the `Value` input box of `SECRET`.\n",
        "4. Toggle the button on the left to allow notebook access to the secret.\n",
        "\n",
        "### Or create a `.env` file\n",
        "\n",
        "Create a `.env` file similar to\n",
        "\n",
        "```shell\n",
        "ANTHROPIC_API_KEY=\"<your-anthropic-api-key>\"\n",
        "OPENAI_API_KEY=\"<your-openai-api-key>\"\n",
        "WANDB_API_KEY=\"<your-wandb-api-key>\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hu7OtFu3jekl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from text_formatting import render\n",
        "from set_env import set_env\n",
        "set_env(\"ANTHROPIC_API_KEY\")\n",
        "set_env(\"WANDB_API_KEY\")\n",
        "set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e8WkuQrkGvY"
      },
      "source": [
        "Welcome to the Prompt Engineering course with Weights and Biases, led by Anish Shah. This course is designed to explore the fascinating world of prompt engineering, a crucial aspect of interacting with and leveraging the capabilities of large language models (LLMs). Throughout this session, we'll dive into various techniques for crafting effective prompts that can significantly enhance the performance of LLMs across a wide range of tasks.\n",
        "\n",
        "Whether you're new to AI and machine learning or looking to deepen your understanding of prompt engineering, this course will provide you with valuable insights and practical skills. By the end of this session, you'll be equipped to design and implement prompts that effectively communicate your intentions to LLMs, enabling more accurate and relevant responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVUlIvXxZihu"
      },
      "source": [
        "This section of the notebook focuses on setting up the environment and installing the required libraries:\n",
        "\n",
        "- It installs the [W&B Weave](https://wandb.github.io/weave/?utm_source=github&utm_medium=course&utm_campaign=prompting) library which is used for tracking llm model operations\n",
        "- It installs `litellm` which is used to standardize model interaction and also make it easy to swap model providers\n",
        "- Some accessory functions are provided for better rendering and environment variable setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RscB2rZ5Zihu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import weave\n",
        "import litellm\n",
        "completion = litellm.completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDA4IggvZihu"
      },
      "source": [
        "## Model and Prompt Configuration\n",
        "The code snippets define important configuration variables for the prompting course:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fRd6P-kdZihu"
      },
      "outputs": [],
      "source": [
        "# These variables store the names of different language models from Anthropic and OpenAI.\n",
        "# The \"SMART\" models (`claude-3-opus` and `gpt-4-turbo`) are more capable but slower,\n",
        "# while the \"FAST\" models (`claude-3-haiku` and `gpt-3.5-turbo`) are faster but less powerful.\n",
        "ANTHROPIC_SMART_MODEL_NAME = \"claude-3-opus-20240229\"\n",
        "ANTHROPIC_FAST_MODEL_NAME = \"claude-3-haiku-20240307\"\n",
        "OPENAI_SMART_MODEL_NAME = \"gpt-4-turbo-2024-04-09\"\n",
        "OPENAI_FAST_MODEL_NAME = \"gpt-3.5-turbo\"\n",
        "\n",
        "# These variables point to two different markdown files containing prompt engineering guides.\n",
        "# `AMAN_PROMPT_GUIDE` refers to Aman Chadha's guide, while `LILIAN_PROMPT_GUIDE` refers to Lilian Weng's guide.\n",
        "AMAN_PROMPT_GUIDE = \"aman_prompt_engineering.md\"\n",
        "LILIAN_PROMPT_GUIDE = \"lilianweng_prompt_engineering.md\"\n",
        "\n",
        "# Here, the `MODEL_NAME` variable is set to use Anthropic's fast model (`claude-3-haiku`),\n",
        "# and the `PROMPT_GUIDE` variable selects Lilian Weng's prompt engineering guide.\n",
        "MODEL_NAME = OPENAI_SMART_MODEL_NAME\n",
        "PROMPT_GUIDE = LILIAN_PROMPT_GUIDE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZttR_xIZihu"
      },
      "source": [
        "These configuration variables allow course participants to easily switch between different models and prompt guides throughout the course by modifying the assigned values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XyZZm2DZihu"
      },
      "source": [
        "## Initializing Weave\n",
        "\n",
        "This line initializes the [W&B Weave library](https://wandb.github.io/weave/?utm_source=github&utm_medium=course&utm_campaign=prompting) - Weave is a toolkit for developing Generative AI applications, providing features like logging, debugging, evaluations, and organization of LLM workflows.\n",
        "\n",
        "Initializing Weave at the start allows you to leverage its capabilities throughout your project, such as decorating Python functions with `@weave.op()` to enable automatic tracing and versioning.\n",
        "\n",
        "By specifying the project name below, you are setting up a dedicated workspace for this course within Weave. This helps keep the course-related experiments, models, and data organized and separate from other projects.\n",
        "\n",
        "Weave brings structure and best practices to the experimental nature of Generative AI development, making it easier to track, reproduce, and share your work. Initializing it early in the notebook ensures you can take full advantage of its features as you progress through the course.\n",
        "\n",
        "To get started, you'll need to sign up for a [Weights & Biases account here](https://wandb.ai/site/?utm_source=github&utm_medium=course&utm_campaign=prompting). When you run `weave.init` below you'll be prompted for your W&B API key which you can find [here](https://wandb.ai/authorize?utm_source=github&utm_medium=course&utm_campaign=prompting). Copy & paste it into the input box below when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uzV38KVSby6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229ae337-f069-4578-a6f8-9a22d5a13fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in as Weights & Biases user: imvenkata.\n",
            "View Weave data at https://wandb.ai/imvenkata-ubs/beginner-llm-prompting-course/weave\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<weave.trace.weave_client.WeaveClient at 0x780fac48a230>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "weave.init(\"beginner-llm-prompting-course\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nsGxSlfZihv"
      },
      "source": [
        "## Defining the get_completion function\n",
        "\n",
        "This code defines a function called `get_completion` that is decorated with `@weave.op()`. The `@weave.op()` decorator is provided by Weave and enables automatic tracing and versioning of the function.\n",
        "\n",
        "The `get_completion` function takes several parameters:\n",
        "- `system_message`: The system message to provide context or instructions to the language model.\n",
        "- `messages`: A list of messages representing the conversation history.\n",
        "- `model_name`: The name of the language model to use (defaults to `MODEL_NAME`).\n",
        "- `max_tokens`: The maximum number of tokens to generate in the response (defaults to 4096).\n",
        "- `temperature`: The sampling temperature for controlling the randomness of the generated text (defaults to 0).\n",
        "\n",
        "Inside the function, it calls the `completion` function (from the `litellm` library) with the provided parameters to generate a completion from the language model. The `temperature` parameter is set to 0, which is recommended for evaluations and RAG (Retrieval-Augmented Generation) systems to ensure deterministic results.\n",
        "\n",
        "The generated response is then printed as JSON using `response.json()`, and the JSON response is returned by the function.\n",
        "\n",
        "By using the `@weave.op()` decorator, Weave automatically tracks and versions the inputs and outputs of the `get_completion` function, making it easier to reproduce and analyze the results later in the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LfuTRXK0b1vm"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def get_completion(system_message: str, messages: list, model: str, max_tokens: int = 4096, temperature: float = 0, **kwargs) -> dict:\n",
        "    \"\"\"\n",
        "    Generates a completion using the specified model, taking into account the system message, conversation history, and additional arguments.\n",
        "\n",
        "    Parameters:\n",
        "        system_message (str): A message providing context or instructions for the model.\n",
        "        messages (list): A list of dictionaries representing the conversation history, where each dictionary has keys 'role' and 'content'.\n",
        "        model (str): The identifier of the model to use for generating completions.\n",
        "        max_tokens (int, optional): The maximum number of tokens to generate in the completion. Defaults to 4096.\n",
        "        temperature (float, optional): The sampling temperature to control the randomness of the generated text. Defaults to 0.\n",
        "        **kwargs: A dictionary of additional keyword arguments. Expected keys include 'system_message', 'model', 'max_tokens', and 'temperature'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing the generated completion as JSON.\n",
        "    \"\"\"\n",
        "    # Adjust messages format based on the model type\n",
        "    if \"gpt\" in model.lower():\n",
        "        formatted_messages = [{\"role\": \"system\", \"content\": system_message}] + messages\n",
        "    else:\n",
        "        kwargs[\"system\"] = system_message  # For non-gpt models, use system_message directly in kwargs\n",
        "\n",
        "    # Common arguments for the completion function\n",
        "    completion_args = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": formatted_messages if \"gpt\" in model.lower() else messages,\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "    # Generate and return the completion\n",
        "    response = completion(**completion_args)\n",
        "    return response.json()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BzE4trkZihv"
      },
      "source": [
        "## Use Case: Building a Prompting Assistant\n",
        "\n",
        "In this section, we explore the practical application of prompt engineering by building a bot that helps users understand prompting techniques and answers questions based on the provided information. This use case demonstrates the power of prompt engineering in creating helpful AI assistants that can make complex topics more accessible and engaging.\n",
        "\n",
        "By leveraging the knowledge contained in a comprehensive guide on prompting techniques, we can develop a bot that provides accurate and relevant answers to user queries. Through careful crafting of system messages and prompt templates, we ensure that the bot's responses are not only informative but also easy to understand, even for beginners.\n",
        "\n",
        "Throughout this use case, course participants will learn how to:\n",
        "\n",
        "- Incorporate context to improve the relevance and accuracy of the model's responses\n",
        "- Use system messages to guide the model's behavior and output style\n",
        "- Standardize inputs and outputs for consistent and reusable prompting assistants\n",
        "- Experiment with different configurations to optimize the bot's performance\n",
        "\n",
        "By engaging with this use case, participants will gain hands-on experience in applying prompt engineering techniques to build a practical and helpful AI assistant. They will develop a deeper understanding of how to effectively communicate with language models and tailor their outputs to specific audiences and use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzBfGidMZihv"
      },
      "source": [
        "### Step 1: Raw Prompting\n",
        "\n",
        "We start by sending a question to the language model without any additional context, using a basic `prompt_llm` function. This demonstrates the model's limitations when lacking the necessary information to provide relevant answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_ljpM75eZihv"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def prompt_llm(question: str, **kwargs) -> str:\n",
        "    \"\"\"\n",
        "    Sends a question to the language model and returns its response.\n",
        "\n",
        "    This function prepares a message with the user's question, handles additional\n",
        "    arguments for the language model, and invokes the get_completion function to\n",
        "    obtain a response. The response's content is then returned.\n",
        "\n",
        "    Parameters:\n",
        "        question (str): The question intended for the language model.\n",
        "        **kwargs: A dictionary of additional keyword arguments. Expected keys include 'system_message', 'model', 'max_tokens', and 'temperature'.\n",
        "\n",
        "    Returns:\n",
        "        str: The language model's response to the question.\n",
        "    \"\"\"\n",
        "    # Prepare the user's question for the language model\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "    # Extract additional parameters, applying defaults if necessary\n",
        "    system_message = kwargs.pop('system_message', \"\")\n",
        "    model = kwargs.pop('model', MODEL_NAME)\n",
        "    max_tokens = kwargs.pop('max_tokens', 4096)\n",
        "    temperature = kwargs.pop('temperature', 0)\n",
        "\n",
        "    # Compile arguments for the completion request\n",
        "    completion_args = {\n",
        "        \"system_message\": system_message,\n",
        "        \"messages\": messages,\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    completion_args.update(kwargs)  # Properly include any other additional arguments\n",
        "\n",
        "    # Request a completion from the language model\n",
        "    response = get_completion(**completion_args)\n",
        "\n",
        "    # Extract and return the content of the model's response\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34VfxBsCZihv",
        "outputId": "60e38a0b-7273-4ce3-ebb5-3f6130eb1ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/imvenkata-ubs/beginner-llm-prompting-course/r/call/01921375-7157-7450-9d0a-ae0f353e2c82\n"
          ]
        }
      ],
      "source": [
        "raw_prompt_response = prompt_llm(\n",
        "    \"Explain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "V0MhfntuZihv",
        "outputId": "08939cdb-e7ea-4776-e22e-bd6bacb4ee9a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "As of my last update in 2023, prompting techniques in AI, particularly in the context of language\n",
              "models like OpenAI's GPT series, have evolved significantly. These techniques are designed to\n",
              "improve the interaction with AI models, enhancing their ability to understand and generate more\n",
              "accurate, relevant, and contextually appropriate responses. Here are some of the latest prompting\n",
              "techniques along with examples for each:  1. **Zero-Shot Learning**:    - **Description**: This\n",
              "technique involves presenting a task to the model without any prior specific training on that task.\n",
              "The model uses its pre-trained knowledge to generate a response.    - **Example**: Asking the model,\n",
              "\"What is the capital of France?\" without having explicitly trained it on geographical facts.  2.\n",
              "**Few-Shot Learning**:    - **Description**: This involves giving the model a few examples to\n",
              "demonstrate the task before asking it to perform on a new example. This helps the model understand\n",
              "the context or the type of response expected.    - **Example**:       ```      Q: Who is the\n",
              "president of the United States? A: Joe Biden.      Q: Who is the prime minister of the United\n",
              "Kingdom? A: Rishi Sunak.      Q: Who is the chancellor of Germany? A: ?      ```      Here, the\n",
              "model uses the pattern from the examples to answer the question about Germany.  3. **Chain of\n",
              "Thought Prompting**:    - **Description**: This technique involves prompting the model to generate\n",
              "intermediate steps or reasoning paths before arriving at a final answer. It helps in complex\n",
              "problem-solving tasks.    - **Example**:       ```      Question: If I have 5 apples and you give me\n",
              "2 more, how many do I have?      Thought Process: Start with 5 apples. Add 2 apples to the 5 apples.\n",
              "5 + 2 = 7.      Answer: 7 apples.      ```  4. **Prompt Engineering or Prompt Crafting**:    -\n",
              "**Description**: This is a more sophisticated form of prompting where the prompts are carefully\n",
              "designed to elicit specific types of responses or behaviors from the model. This can involve using\n",
              "specific keywords, phrases, or structured prompts.    - **Example**:       ```      To generate a\n",
              "poem about the ocean:      \"Write a four-line poem where the first line starts with 'The mighty\n",
              "ocean', the second line includes 'waves', the third line mentions 'moonlight', and the fourth line\n",
              "ends with 'tides'.\"      ```  5. **Instruction Following**:    - **Description**: This technique\n",
              "involves giving direct, clear instructions to the model about what is expected in the response. This\n",
              "is particularly useful for tasks requiring specific formats or data handling.    - **Example**:\n",
              "```      \"Summarize the following article in three bullet points focusing on the main thesis, key\n",
              "evidence, and conclusion.\"      ```  6. **Soft Prompts or Embedding Prompts**:    - **Description**:\n",
              "Unlike traditional hard prompts that use textual input, soft prompts involve tweaking the input\n",
              "embeddings directly to guide the model's responses. This can be more flexible and powerful but\n",
              "requires understanding of the model's embedding space.    - **Example**: Adjusting embeddings to\n",
              "bias the model towards generating technical versus layman explanations.  7. **Hybrid Prompting**:\n",
              "- **Description**: Combining various prompting techniques to handle complex tasks or to improve\n",
              "model performance across different types of tasks.    - **Example**: Using a few-shot example to set\n",
              "context, followed by a chain of thought prompt for detailed problem-solving, and concluding with an\n",
              "instruction-following prompt for summarization.  These techniques represent the forefront of how\n",
              "researchers and practitioners interact with advanced AI models to leverage their capabilities\n",
              "effectively. Each technique can be tailored to specific applications and needs, enhancing both the\n",
              "utility and accessibility of AI technologies."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(raw_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IOkXJ7mZihv"
      },
      "source": [
        "The model's response to the raw prompt is inadequate because it lacks the necessary context to provide a meaningful answer. Without any background information or specific details about prompting techniques, the model can only generate a generic, high-level response that fails to address the question effectively, many times providing no response at all.\n",
        "\n",
        "This poor performance highlights the importance of providing relevant context when prompting language models. By supplying the model with additional information related to the topic at hand, we can guide it towards generating more accurate, detailed, and useful responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-Z_dJovZihv"
      },
      "source": [
        "### Step 2. Prompting with Context\n",
        "\n",
        "We can provide the necessary context to the language model by including it directly alongside the question. In this example, we use a comprehensive guide on prompting techniques written by [Lilian Weng](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/). This guide is particularly useful as it condenses many great papers and articles into a single-page resource, covering various prompting techniques.\n",
        "\n",
        "To incorporate the context, we:\n",
        "\n",
        "1. Load the markdown file containing the prompting guide using the `load_markdown_file` function.\n",
        "2. Concatenate the loaded context with the question in the `context_prompt_response` variable.\n",
        "3. Pass the combined context and question to the `prompt_llm` function to generate a response.\n",
        "\n",
        "By providing the model with relevant context, we expect to receive more accurate and informative answers to our questions about prompting techniques.\n",
        "\n",
        "Note: As an alternative, you can also use a more extensive guide by [Aman Chadha](https://aman.ai/primers/ai/prompt-engineering/) for additional context and information on prompt engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7MSLttr_Zihv"
      },
      "outputs": [],
      "source": [
        "def load_markdown_file(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Reads and returns the content of a markdown file specified by its path.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): The path to the markdown file to be read.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the markdown file as a string.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        markdown_content = file.read()\n",
        "    return markdown_content\n",
        "context = load_markdown_file(PROMPT_GUIDE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOczV4JUZihv"
      },
      "source": [
        "Note: Anthropic has an amazingly large context size and as a result we can luckily just shove the whole document into the prompt in this situation. This can get quite expensive however so it typically makes more sense to use techniques that chunk the document into better sizes or use a RAG based pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvQfYnDyZihv",
        "outputId": "5db43b3e-c31d-4d2b-9a67-9f116a58aedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/imvenkata-ubs/beginner-llm-prompting-course/r/call/0192137e-1fec-7991-9d8e-beebb6d9bc19\n"
          ]
        }
      ],
      "source": [
        "context_prompt_response = prompt_llm(\n",
        "    context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "rG1WAJXFZihv",
        "outputId": "5fa69efd-f116-4076-de1f-22c8dc74d165"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Prompt engineering has evolved significantly with the advent of large language models (LLMs),\n",
              "offering various techniques to enhance the interaction and output quality of these models. Below,\n",
              "I'll explain some of the latest prompting techniques and provide examples for each.  ### 1. **Zero-\n",
              "Shot Prompting** Zero-shot prompting involves presenting a task to the model without any prior\n",
              "examples or context, relying solely on the model's pre-trained knowledge.  **Example:** ``` Prompt:\n",
              "\"Translate the following sentence into French: 'Hello, how are you?'\" Model Output: \"Bonjour,\n",
              "comment ça va?\" ```  ### 2. **Few-Shot Prompting** Few-shot prompting provides the model with a few\n",
              "examples to help it understand the task context and expected output format better.  **Example:** ```\n",
              "Prompt: - English: \"I am happy.\"   French: \"Je suis heureux.\" - English: \"She is running.\"   French:\n",
              "\"Elle court.\" - English: \"They are watching a movie.\"   French: \"Ils regardent un film.\" - English:\n",
              "\"Translate this sentence into French: 'What are you doing?'\" Model Output: \"Que faites-vous?\" ```\n",
              "### 3. **Chain-of-Thought (CoT) Prompting** Chain-of-Thought prompting encourages the model to\n",
              "generate intermediate reasoning steps before arriving at a final answer, particularly useful for\n",
              "complex reasoning tasks.  **Example:** ``` Prompt: \"Tom has 5 apples and gives 3 to Jane. How many\n",
              "apples does Tom have now? Let's think step by step.\" Model Output: \"Tom starts with 5 apples. He\n",
              "gives 3 to Jane. 5 - 3 = 2. So, Tom has 2 apples now.\" ```  ### 4. **Self-Consistency Sampling**\n",
              "This technique involves generating multiple answers from the model (usually by varying parameters\n",
              "like temperature), then selecting the most common answer as the final output to improve reliability.\n",
              "**Example:** ``` Prompt: \"What is the capital of France?\" Generated Outputs: [\"Paris\", \"Paris\",\n",
              "\"Lyon\", \"Paris\", \"Marseille\"] Final Output: \"Paris\" (most frequent answer) ```  ### 5. **Instruction\n",
              "Prompting** Instruction prompting involves directly stating the task in the prompt, guiding the\n",
              "model to focus on specific instructions without providing examples.  **Example:** ``` Prompt:\n",
              "\"Please summarize the following text in one sentence: 'The quick brown fox jumps over the lazy\n",
              "dog.'\" Model Output: \"A quick brown fox leaps over a lazy dog.\" ```  ### 6. **Retrieval-Augmented\n",
              "Prompting** This technique combines external information retrieval with model prompting, where the\n",
              "model first retrieves relevant information before generating an answer.  **Example:** ``` Prompt:\n",
              "\"What is the latest research on climate change? Retrieve recent articles first.\" Retrieved\n",
              "Information: [Summary of recent climate change research articles] Model Output: \"Recent research on\n",
              "climate change focuses on the impact of rising temperatures on polar ice caps and global sea\n",
              "levels.\" ```  ### 7. **Programmatic Prompting** In programmatic prompting, the model generates code\n",
              "or commands that can be executed to solve a problem, often used in conjunction with code execution\n",
              "environments.  **Example:** ``` Prompt: \"Write a Python function to calculate the factorial of a\n",
              "number.\" Model Output: ``` ```python def factorial(n):     if n == 0:         return 1     else:\n",
              "return n * factorial(n-1) ``` ```  Each of these techniques leverages the capabilities of LLMs in\n",
              "different ways, tailored to specific tasks or desired outcomes. By understanding and applying these\n",
              "methods, users can significantly enhance the effectiveness of their interactions with language\n",
              "models."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(context_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1FRnUIyZihv"
      },
      "source": [
        "This response is a significant improvement! We can see that by providing the model with relevant context, it can generate an answer that includes details about various prompting techniques covered in the course. The model effectively utilizes the information from the provided guide to address the question more comprehensively.\n",
        "\n",
        "However, there is still room for improvement. The model tends to regurgitate the technical information from the guide without simplifying or explaining the concepts in an easily understandable manner. The response may be too complex or jargon-heavy for beginners or those new to the topic of prompt engineering.\n",
        "\n",
        "To address this issue, we need to guide the model towards providing explanations that are more accessible and beginner-friendly. This is where the next step of conditioning the model's responses with a carefully crafted system prompt comes into play. By instructing the model to break down the technical details and present the information in a more digestible format, we can ensure that the responses are not only informative but also easy to understand for a broader audience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcY2utPTZihv"
      },
      "source": [
        "### Step 3. Condition Responses with a System Prompt\n",
        "\n",
        "To ensure that the bot explains the information in a way that is easy to understand, we can provide a system prompt that guides the model to present the content in a beginner-friendly manner. Here’s why this system prompt is effective:\n",
        "\n",
        "1. **Objective Clarity**: It directly states the task — simplifying prompt engineering concepts with examples. This aligns with the principle of having a clear and specific objective, which helps the LLM focus on the exact task ([source](https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca)).\n",
        "\n",
        "2. **Tone Specification**: Setting a friendly and educational tone guides the LLM on the desired interaction style, making the information approachable and digestible.\n",
        "\n",
        "3. **Context Awareness**: By acknowledging the user's basic AI knowledge, the prompt tailors the complexity of the content, ensuring it is suitable for beginners without being overly simplistic.\n",
        "\n",
        "4. **Guidance on Style**: Instructing the use of analogies and simple examples helps in breaking down complex topics into understandable segments, which is crucial for teaching technical subjects effectively.\n",
        "\n",
        "5. **Verification of Output**: Emphasizing clarity and relevance ensures that the responses are not only correct but also useful and directly applicable to the user’s needs.\n",
        "\n",
        "6. **Highlighting Benefits**: Mentioning the benefits of simplifying technical concepts engages users by showing the value of what they are learning, enhancing their motivation and the educational impact.\n",
        "\n",
        "### General Approach to Constructing Effective System Prompts\n",
        "\n",
        "When constructing system prompts for LLMs, consider the following steps to ensure effectiveness and clarity:\n",
        "\n",
        "- **Define the Objective**: Clearly state what you want the LLM to achieve. This should be specific and concise.\n",
        "- **Set the Tone and Style**: Indicate how the response should feel or sound. This helps the LLM adjust its language and approach.\n",
        "- **Provide Necessary Context**: Include any background information that will help the LLM understand the scope and depth of the response required.\n",
        "- **Incorporate Guidance for Content**: Direct the LLM on how to structure its response or what elements to include, such as examples or analogies.\n",
        "- **Specify Output Format**: If necessary, define how the response should be formatted. This is particularly important for tasks requiring a specific output structure.\n",
        "- **Use Clear and Direct Language**: Avoid ambiguity by using straightforward and direct language. This reduces the chances of misinterpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OUxrPlacZihw"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "Objective: Simplify prompt engineering concepts for easy understanding. Provide clear examples for each technique.\n",
        "Tone: Friendly and educational, suitable for beginners.\n",
        "Context: Assume basic AI knowledge; avoid deep technical jargon.\n",
        "Guidance: Use metaphors and simple examples to explain concepts. Keep explanations concise and applicable.\n",
        "Verification: Ensure clarity and relevance in responses, with practical examples.\n",
        "Benefits: Help users grasp prompt engineering basics, enhancing their AI interaction experience.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us_bNQ9cZihw",
        "outputId": "e887bf7f-2418-4757-9960-06cda79b9c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/imvenkata-ubs/beginner-llm-prompting-course/r/call/01921386-1021-7fc1-b147-46add5bc6004\n"
          ]
        }
      ],
      "source": [
        "system_and_context_prompt_response = prompt_llm(\n",
        "    system_message=system_message,\n",
        "    question=context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "MHwzlWYtZihw",
        "outputId": "94d711c3-4121-47e6-f0be-b301ffbe350e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Prompt engineering is like giving your AI a recipe to follow when cooking up answers. It's about\n",
              "crafting the right questions or instructions to get the most accurate and useful responses. Let's\n",
              "break down some of the latest techniques in prompt engineering and provide examples for each:  ###\n",
              "1. **Zero-Shot Learning** Imagine you've never baked a cake before, and someone asks you to make one\n",
              "without any recipe or prior experience. That's zero-shot learning. You provide the AI with a task it\n",
              "has never seen before and ask for a response based on its pre-existing knowledge.  **Example:** ```\n",
              "Prompt: \"Explain the theory of relativity.\" Response: [AI generates an explanation based on its\n",
              "training] ```  ### 2. **Few-Shot Learning** This is like having a few recipes as guides before you\n",
              "try to bake a new cake. You show the AI several examples of the task done correctly, and then ask it\n",
              "to perform a similar task.  **Example:** ``` Prompt: 1. \"Translate 'Hello, how are you?' into\n",
              "French.\" Response: \"Bonjour, comment ça va?\" 2. \"Translate 'See you later!' into French.\" Response:\n",
              "\"À plus tard!\" 3. \"Translate 'Where is the library?' into French.\" Response: [AI generates the\n",
              "translation] ```  ### 3. **Chain-of-Thought (CoT)** Think of this as breaking down a complex recipe\n",
              "into step-by-step instructions. The AI is prompted to think aloud, showing its reasoning step by\n",
              "step before giving the final answer. This is especially useful for complex problem-solving.\n",
              "**Example:** ``` Prompt: \"If a car travels 60 miles in 1.5 hours, what is its average speed?\"\n",
              "Response: \"First, find the speed by dividing the distance by time. Speed = 60 miles / 1.5 hours = 40\n",
              "mph. So, the average speed is 40 mph.\" ```  ### 4. **Self-Consistency Sampling** This technique is\n",
              "like trying to bake several cakes and then choosing the best one. The AI generates multiple answers\n",
              "and selects the most consistent or probable one.  **Example:** ``` Prompt: \"What is the capital of\n",
              "France?\" Responses: \"Paris\", \"Paris\", \"Lyon\", \"Paris\" Selected Response: \"Paris\" (as it appears most\n",
              "consistently) ```  ### 5. **Instruction Prompting** Here, you give clear, direct instructions to the\n",
              "AI, much like a straightforward recipe. This method is about being explicit about what you want the\n",
              "AI to do.  **Example:** ``` Prompt: \"Write a summary of the article provided below in three\n",
              "sentences.\" Response: [AI generates a three-sentence summary] ```  ### 6. **Automatic Prompt\n",
              "Design** This advanced technique involves using algorithms to automatically generate and optimize\n",
              "prompts. It's like having a machine that can create its own recipes based on what ingredients (data)\n",
              "you have.  **Example:** ``` Initial Prompt: \"Describe the benefits of exercise.\" AI-Generated\n",
              "Prompt: \"List three health advantages of regular physical activity.\" Response: [AI lists benefits]\n",
              "```  ### 7. **Augmented Language Models** These models are enhanced with additional capabilities\n",
              "like accessing external databases or performing specific computations. It's akin to a chef who uses\n",
              "a calculator or a reference book to ensure the recipe is perfect.  **Example:** ``` Prompt: \"What is\n",
              "the current population of New York City?\" Response: [AI retrieves the most recent data and responds]\n",
              "```  Each of these techniques helps in guiding the AI more effectively to achieve better, more\n",
              "accurate outputs. By understanding and using these methods, you can significantly enhance your\n",
              "interactions with AI models."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(system_and_context_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMywIL3RZihw"
      },
      "source": [
        "Great! Now we're able to get a response that is easy to understand and provides a lot of context. The model has successfully broken down the technical concepts into beginner-friendly explanations, using simple language, analogies, and examples to convey the key points. This approach makes the information more accessible and engaging for those new to prompt engineering, fostering a deeper understanding of the subject matter.\n",
        "\n",
        "The next step is to standardize the inputs and outputs in a way that allows us to ask different questions and pass different context in the future. By creating a consistent structure for our prompts and responses, we can streamline the development of LLM applications and make it easier to experiment with various configurations. This standardization will enable us to quickly iterate on our prompts, test different contexts, and fine-tune our models to achieve the best possible results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrPmtoHJZihw"
      },
      "source": [
        "### Step 4: System Prompts - Inputs\n",
        "\n",
        "To make it easier to experiment with different parameters and retrieve our best models, we can wrap our prompting logic in a collection of modular functions decorated with `@weave.op()`. This allows us to track and version our operations, making it easier to reproduce and analyze our results. We can also define a standard input structure for our system prompts, ensuring consistency across different prompts and use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RaBi-WsgZihw"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"{context}\\n{question}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "D7RH2vBfZihw"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def format_prompt(prompt_template: str, **kwargs):\n",
        "    \"\"\"\n",
        "    Formats a prompt template with provided keyword arguments.\n",
        "\n",
        "    This function takes a template string and a dictionary of keyword arguments,\n",
        "    then formats the template string using these arguments.\n",
        "\n",
        "    Parameters:\n",
        "        prompt_template (str): The template string to be formatted.\n",
        "        **kwargs (dict): Keyword arguments to format the template string with.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted prompt template.\n",
        "    \"\"\"\n",
        "    return prompt_template.format(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Bf42sc1NZihw"
      },
      "outputs": [],
      "source": [
        "# In this app we assume only a context and question are passed to the prompt template but that need not be true\n",
        "@weave.op()\n",
        "def llm_app(prompt_template: str, context: str, question: str, **kwargs):\n",
        "    \"\"\"\n",
        "    Generates a response using a formatted prompt based on a template, context, and question.\n",
        "\n",
        "    This function formats a given prompt template with the specified context and question, then\n",
        "    generates a response using the prompt_llm function with additional keyword arguments.\n",
        "\n",
        "    Parameters:\n",
        "        prompt_template (str): The template string used to format the prompt.\n",
        "        context (str): The context information to be included in the prompt.\n",
        "        question (str): The specific question to be asked in the prompt.\n",
        "        **kwargs (dict): Additional keyword arguments to be passed to the prompt_llm function.\n",
        "\n",
        "    Returns:\n",
        "        str: A string representing the generated response.\n",
        "    \"\"\"\n",
        "    formatted_prompt = format_prompt(prompt_template=prompt_template, context=context, question=question)\n",
        "    response = prompt_llm(\n",
        "        question=formatted_prompt,\n",
        "        **kwargs\n",
        "    )\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yLkByBgZihw"
      },
      "source": [
        "We defined our llm_app which in turns acts as the core of our prompting assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MkED38yQZihw"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "Explain the differences between chain of thought and Self-Consistency Sampling\n",
        "prompting techniques? Please provide a clear explanation and a practical example\n",
        "for each technique within a structured format.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnFMMm0aZihz",
        "outputId": "0eae955c-f43e-4ebf-f045-1f5917ff5b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/imvenkata-ubs/beginner-llm-prompting-course/r/call/0192138a-ce8d-7b51-9a43-797397b3f298\n"
          ]
        }
      ],
      "source": [
        "input_template_response = llm_app(\n",
        "    system_message=system_message,\n",
        "    prompt_template=prompt_template,\n",
        "    context=context,\n",
        "    question=question,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "u0lHotvjZihz",
        "outputId": "19577a29-d1b3-44c0-f81e-51cf1c83f1cb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "### Chain of Thought (CoT) Prompting  **Explanation:** Chain of Thought (CoT) prompting is a\n",
              "technique used to guide large language models (LLMs) to solve complex reasoning tasks by explicitly\n",
              "asking them to generate intermediate reasoning steps before arriving at a final answer. This method\n",
              "encourages the model to \"think aloud\" by breaking down the problem into smaller, manageable parts,\n",
              "which are sequentially addressed to build up to the solution.  **Example:** Imagine you're asking an\n",
              "AI to solve the following math problem:  **Problem:** \"Alice has 10 apples. She gives 3 to Bob and\n",
              "then receives 5 more from Carol. How many apples does Alice have now?\"  **CoT Prompt:** ``` Let's\n",
              "think step by step. Alice starts with 10 apples. She gives 3 to Bob, so 10 - 3 = 7 apples. Then,\n",
              "Carol gives her 5 more apples. So, 7 + 5 = 12 apples. Therefore, Alice has 12 apples now. ```  In\n",
              "this example, the AI breaks down the problem into smaller steps, articulating each part of the\n",
              "process, which helps in understanding the logic behind the final answer.  ### Self-Consistency\n",
              "Sampling  **Explanation:** Self-Consistency Sampling is a technique used to enhance the reliability\n",
              "of answers provided by LLMs, especially in tasks where multiple plausible answers might exist. This\n",
              "method involves generating multiple answers from the model under slightly varied conditions or using\n",
              "different random seeds, and then selecting the most common answer (majority vote) or the most\n",
              "plausible one based on additional criteria.  **Example:** Suppose you want to know the sentiment of\n",
              "a movie review: \"The movie was breathtaking and beautifully shot, but the plot was predictable and\n",
              "lacked depth.\"  **Self-Consistency Sampling Prompt:** You would prompt the model multiple times,\n",
              "slightly altering the phrasing or the context window each time, to generate sentiments:  1. **Prompt\n",
              "1:** \"Considering the description, what is the sentiment of this review?\" 2. **Prompt 2:** \"Is the\n",
              "review positive, negative, or neutral based on the content provided?\" 3. **Prompt 3:** \"What\n",
              "sentiment does the following review express about the movie?\"  **Generated Responses:** 1. Positive\n",
              "2. Positive 3. Neutral  In this case, since 'Positive' is the most frequently occurring response, it\n",
              "would be selected as the final answer, assuming the model sees the positive aspects as more\n",
              "significant than the negative.  **Comparison:** - **Chain of Thought (CoT):** Focuses on breaking\n",
              "down the reasoning process into explicit steps, enhancing transparency and understanding in problem-\n",
              "solving tasks. - **Self-Consistency Sampling:** Aims at improving answer reliability by generating\n",
              "multiple responses and choosing the most consistent or plausible one, useful in reducing randomness\n",
              "and error in model outputs.  Both techniques enhance the model's utility in different scenarios,\n",
              "with CoT being more about clarity in reasoning and Self-Consistency about accuracy and reliability\n",
              "in outputs."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(input_template_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXK2qB2xZihz"
      },
      "source": [
        "Now we can easily and consistently swap system messages, context, and questions to get the best results. This allows us to test various combinations of system messages, context, and questions to find the most effective prompts for our specific use case.\n",
        "\n",
        "However, as we can see from the example, the current system prompt doesn't work as well with the newly asked question. This highlights the importance of tailoring the system prompt to the specific task at hand. Additionally, we may want to enforce more consistency in the format of our model's outputs. While using third-party packages like Instructor is beyond the scope of this course, we can achieve similar results by using proper tags in our prompt. By including specific tags or formatting instructions in the prompt, we can guide the model to respond in a way that is more consistent and easier to parse on our end\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awi7l65xZihz"
      },
      "source": [
        "### Step 5: System Prompts - Outputs\n",
        "\n",
        "In this step, we focus on improving the consistency and structure of our model's outputs by modifying the prompt template. By including specific tags and formatting instructions in the prompt, we can guide the model to respond in a way that is easier to parse and process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx37wfcVZihz"
      },
      "source": [
        "In the system message, we've added a new tag called `Format` which provides instructions for the model to respond within an <answer></answer> tag, with separate <explanation> and <example> tags for each concept. This structured format helps organize the information and makes it easier to extract and analyze the responses programmatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jamNqMdLZihz"
      },
      "outputs": [],
      "source": [
        "def update_prompt_with_output_indicator(system_message: str, prompt_template: str):\n",
        "    if \"gpt\" in MODEL_NAME:\n",
        "        system_format_msg = \"\"\"\n",
        "    Format: Respond within a structured JSON object, using the keys provided in the prompt to organize your response.\n",
        "    Provide a condensed answer under the 'condensed_answer' key, detailed explanations under 'explanation' keys,\n",
        "    and examples under 'example' keys within each explanation.\n",
        "    \"\"\"\n",
        "        prompt_format_msg = \"\"\"\n",
        "    You must respond in JSON format.\n",
        "    Your response should follow this structure:\n",
        "    {{\n",
        "      \"answer\": {{\n",
        "        \"condensed_answer\": \"CONDENSED_ANSWER\",\n",
        "        \"explanation_1\": {{\n",
        "          \"detail\": \"EXPLANATION_1\",\n",
        "          \"example\": \"EXAMPLE_1\"\n",
        "        }},\n",
        "        \"explanation_2\": {{\n",
        "          \"detail\": \"EXPLANATION_2\",\n",
        "          \"example\": \"EXAMPLE_2\"\n",
        "        }},\n",
        "        ...\n",
        "      }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "    else:\n",
        "        system_format_msg = \"\"\"\n",
        "    Format: Respond within an <answer></answer> tag, with as many <explanation></explanation> tags as needed,\n",
        "    ensuring that the <detail></detail> and <example></example> tags are used within each <explanation></explanation> tag.\n",
        "    Provide a condensed answer for the question in the <condensed_answer></condensed_answer> tag.\n",
        "    \"\"\"\n",
        "        prompt_format_msg = \"\"\"\n",
        "    You must respond within an <answer></answer> XML tags.\n",
        "    Inside of the <answer> markdown tag, you must provide a format of\n",
        "    <answer>\n",
        "        <condensed_answer> CONDENSED_ANSWER </condensed_answer>\n",
        "        <explanation>\n",
        "          <detail> EXPLANATION </detail>\n",
        "          <example> EXAMPLE </example>\n",
        "        </explanation>\n",
        "        <explanation>\n",
        "          <detail> EXPLANATION </detail>\n",
        "          <example> EXAMPLE </example>\n",
        "        </explanation>\n",
        "        ...\n",
        "    </answer>\n",
        "    \"\"\"\n",
        "    formatted_system_message = system_message + \"\\n\" + system_format_msg\n",
        "    formatted_prompt_template = prompt_template + \"\\n\" + prompt_format_msg\n",
        "\n",
        "    return formatted_system_message, formatted_prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Fk4ls2bdZihz"
      },
      "outputs": [],
      "source": [
        "formatted_system_message, formatted_prompt_template = update_prompt_with_output_indicator(system_message, prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBXfcov4Zihz"
      },
      "source": [
        "Similarly, we've updated the `prompt_template` to include the new formatting instructions, ensuring that the model generates responses that adhere to the specified structure. By enforcing a consistent output format, we can streamline the processing of the model's responses and facilitate further analysis and evaluation of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PUiPyePdZihz"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "Explain the differences between zero-shot, few-shot, and chain of thought\n",
        "prompting techniques? Please provide a clear explanation and a practical example\n",
        "for each technique within a structured format.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_kCyiiiZihz",
        "outputId": "90a40e1c-085d-4a6c-e46b-3adacfc107c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/imvenkata-ubs/beginner-llm-prompting-course/r/call/0192138c-e304-74f0-9d19-156c0b08058b\n"
          ]
        }
      ],
      "source": [
        "output_indicator_response = llm_app(\n",
        "    system_message=formatted_system_message,\n",
        "    prompt_template=formatted_prompt_template,\n",
        "    context=context,\n",
        "    question=question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "AcObrMLNZihz",
        "outputId": "97a7885b-51b1-4495-cb8d-05e0fc82ef10"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{   \"answer\": {     \"condensed_answer\": \"Zero-shot, few-shot, and chain-of-thought prompting are\n",
              "techniques used to guide language models in generating responses. Zero-shot involves no prior\n",
              "examples, relying solely on the task description. Few-shot uses a small set of examples to guide the\n",
              "model. Chain-of-thought involves prompting the model to generate a step-by-step reasoning process\n",
              "before arriving at the final answer.\",     \"explanation_1\": {       \"detail\": \"Zero-shot prompting\n",
              "is when the model is given a task without any previous examples or context, relying solely on its\n",
              "pre-trained knowledge to generate a response. This method tests the model's ability to understand\n",
              "and respond based on its initial training alone.\",       \"example\": {         \"prompt\": \"Text: What\n",
              "is the capital of France?\\\\nAnswer:\",         \"response\": \"The capital of France is Paris.\"       }\n",
              "},     \"explanation_2\": {       \"detail\": \"Few-shot prompting provides the model with a small number\n",
              "of examples before presenting the actual task. This helps the model understand the context and the\n",
              "specific type of response expected, improving its accuracy on similar tasks.\",       \"example\": {\n",
              "\"prompt\": \"Text: The large cat sat on the tiny mouse.\\\\nSentiment: Negative\\\\nText: The delicious\n",
              "cake was enjoyed by everyone at the party.\\\\nSentiment: Positive\\\\nText: The movie was boring and\n",
              "too long.\\\\nSentiment:\",         \"response\": \"Negative\"       }     },     \"explanation_3\": {\n",
              "\"detail\": \"Chain-of-thought prompting encourages the model to generate intermediate reasoning steps\n",
              "before arriving at the final answer. This technique is particularly useful for complex reasoning\n",
              "tasks, helping the model to break down the problem and process it step-by-step.\",       \"example\": {\n",
              "\"prompt\": \"Question: If you have 3 apples and you take away 2, how many do you have?\\\\nAnswer: Let's\n",
              "think step by step. You start with 3 apples. You take away 2 apples. So, you are left with 3 - 2 = 1\n",
              "apple.\\\\nTherefore, the answer is 1.\",         \"response\": \"1\"       }     }   } }"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(output_indicator_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQMLayfoZihz"
      },
      "source": [
        "## Advanced Prompting Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wViqyelIZihz"
      },
      "source": [
        "### Zero-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "NT5s5XH8Zihz"
      },
      "outputs": [],
      "source": [
        "def update_with_zero_shot_prompt(question: str):\n",
        "    zero_shot_instruction = \"Without using any specific examples, \"\n",
        "    return zero_shot_instruction + question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3J5pZv2MZihz"
      },
      "outputs": [],
      "source": [
        "zero_shot_question = update_with_zero_shot_prompt(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_5hz5qDZihz",
        "outputId": "0b1aaaf3-4161-4bf2-bbdf-425685a67176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/imvenkata-ubs/beginner-llm-prompting-course/r/call/0192138f-4cd1-7f80-b381-da722b9d4da6\n"
          ]
        }
      ],
      "source": [
        "zero_shot_response = llm_app(\n",
        "    system_message=formatted_system_message,\n",
        "    prompt_template=formatted_prompt_template,\n",
        "    context=context,\n",
        "    question=zero_shot_question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "dZVCNB2-Zihz",
        "outputId": "f21ce9b0-d2be-415c-cf27-4412c1b973ef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{   \"answer\": {     \"condensed_answer\": \"Zero-shot, few-shot, and chain-of-thought prompting are\n",
              "techniques used to guide language models in generating responses. Zero-shot involves no prior\n",
              "examples, relying solely on the prompt. Few-shot uses a small set of examples to guide the model.\n",
              "Chain-of-thought involves prompting the model to generate a step-by-step reasoning process before\n",
              "arriving at the final answer.\",     \"explanation_1\": {       \"detail\": \"Zero-shot prompting is when\n",
              "the model is given a task without any previous examples or context, relying solely on its pre-\n",
              "trained knowledge to generate a response.\",       \"example\": \"Prompt: 'What is the capital of\n",
              "France?' The model directly answers 'Paris' without any prior examples.\"     },     \"explanation_2\":\n",
              "{       \"detail\": \"Few-shot prompting provides the model with a few examples of the task at hand,\n",
              "helping it understand the context and desired output format better.\",       \"example\": \"Prompt:\n",
              "'Translate the following sentences to French: 1. Hello, how are you? - Bonjour, comment ca va? 2. I\n",
              "am fine, thank you. - Je vais bien, merci. 3. What is your name?' The model uses the structure of\n",
              "the provided examples to correctly translate the third sentence.\"     },     \"explanation_3\": {\n",
              "\"detail\": \"Chain-of-thought prompting encourages the model to articulate its reasoning process step-\n",
              "by-step before providing the final answer, enhancing its ability to handle complex reasoning\n",
              "tasks.\",       \"example\": \"Prompt: 'If a car travels 60 miles in 2 hours, what is its average\n",
              "speed?' The model might generate a response like: 'First, find the total distance traveled, which is\n",
              "60 miles. Next, divide this by the total time, which is 2 hours, resulting in an average speed of 30\n",
              "miles per hour.'\"     }   } }"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(zero_shot_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TFCOqEBZihz"
      },
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "dcqLfQQoZihz"
      },
      "outputs": [],
      "source": [
        "def update_with_few_shot_prompt(question: str):\n",
        "    if \"gpt\" in MODEL_NAME:\n",
        "        few_shot_examples = \"\"\"\n",
        "        Here are a few examples of prompting techniques in JSON format:\n",
        "        {{\n",
        "            \"answer\": {{\n",
        "                \"condensed_answer\": \"Different prompting techniques are used to guide language models in generating desired outputs.\",\n",
        "                \"explanation_1\": {{\n",
        "                    \"detail\": \"Translation prompts provide the model with a source language text and request the translation in a target language.\",\n",
        "                    \"example\": \"Translate the following English text to French: 'Hello, how are you?'\"\n",
        "                }},\n",
        "                \"explanation_2\": {{\n",
        "                    \"detail\": \"Sentiment classification prompts ask the model to determine the sentiment expressed in a given text.\",\n",
        "                    \"example\": \"Classify the sentiment of the following text: 'The movie was terrible.'\"\n",
        "                }},\n",
        "                \"explanation_3\": {{\n",
        "                    \"detail\": \"Factual question prompts require the model to provide an answer along with an explanation or reasoning.\",\n",
        "                    \"example\": \"What is the capital of Germany? Explain your reasoning.\"\n",
        "                }}\n",
        "            }}\n",
        "        }}\n",
        "        \"\"\"\n",
        "    else:\n",
        "        few_shot_examples = \"\"\"\n",
        "        Here are a few examples of prompting techniques in XML format:\n",
        "        <answer>\n",
        "            <condensed_answer>Different prompting techniques are used to guide language models in generating desired outputs.</condensed_answer>\n",
        "            <explanation>\n",
        "                <detail>Translation prompts provide the model with a source language text and request the translation in a target language.</detail>\n",
        "                <example>Translate the following English text to Spanish: 'Good morning, how can I help you?'</example>\n",
        "            </explanation>\n",
        "            <explanation>\n",
        "                <detail>Sentiment classification prompts ask the model to determine the sentiment expressed in a given text.</detail>\n",
        "                <example>Classify the sentiment of the following text: 'I love this product!'</example>\n",
        "            </explanation>\n",
        "            <explanation>\n",
        "                <detail>Factual question prompts require the model to provide an answer along with an explanation or reasoning.</detail>\n",
        "                <example>What is the capital of Canada? Provide your thought process.</example>\n",
        "            </explanation>\n",
        "        </answer>\n",
        "        \"\"\"\n",
        "\n",
        "    return few_shot_examples + \"\\n\" + question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "c2ayUtbPZihz"
      },
      "outputs": [],
      "source": [
        "few_shot_question = update_with_few_shot_prompt(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4FJT682Zih0",
        "outputId": "ffc61530-7ec0-40ed-9128-506d9caf7c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/imvenkata-ubs/beginner-llm-prompting-course/r/call/0192138f-cd3e-7a10-b534-cad92c86f8f5\n"
          ]
        }
      ],
      "source": [
        "few_shot_response = llm_app(\n",
        "    system_message=formatted_system_message,\n",
        "    prompt_template=formatted_prompt_template,\n",
        "    context=context,\n",
        "    question=few_shot_question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "UtgpDqxVZih0",
        "outputId": "98c2f36d-ecd3-475f-ac38-ec333aa5b3cd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{   \"answer\": {     \"condensed_answer\": \"Zero-shot, few-shot, and chain of thought (CoT) are\n",
              "prompting techniques that guide language models to generate desired outputs based on different\n",
              "levels of context and instruction.\",     \"explanation_1\": {       \"detail\": \"Zero-shot prompting\n",
              "involves presenting a task to the model without any prior examples or context. The model uses its\n",
              "pre-trained knowledge to generate a response.\",       \"example\": {         \"prompt\": \"Translate the\n",
              "sentence 'Hello, how are you?' to French.\",         \"model_response\": \"Bonjour, comment allez-vous?\"\n",
              "}     },     \"explanation_2\": {       \"detail\": \"Few-shot prompting provides the model with a few\n",
              "examples of the task before presenting the actual query. This helps the model understand the task\n",
              "context and expected output format better.\",       \"example\": {         \"prompt\": \"Given the\n",
              "examples: (1) 'The sky is blue.' - Sentiment: Positive, (2) 'It is raining again.' - Sentiment:\n",
              "Negative. Classify the sentiment of the following text: 'What a wonderful day!'\",\n",
              "\"model_response\": \"Sentiment: Positive\"       }     },     \"explanation_3\": {       \"detail\": \"Chain\n",
              "of Thought (CoT) prompting involves instructing the model to generate a step-by-step reasoning\n",
              "process before arriving at the final answer. This is particularly useful for complex reasoning\n",
              "tasks.\",       \"example\": {         \"prompt\": \"Solve the problem: If you have 3 apples and you buy 5\n",
              "more, how many apples do you have in total? Explain your reasoning.\",         \"model_response\": \"You\n",
              "start with 3 apples. If you buy 5 more, you add 5 to 3, making 8 apples in total. So, the answer is\n",
              "8.\"       }     }   } }"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(few_shot_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm0Pn_0BZih0"
      },
      "source": [
        "### Chain of Thought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bWm1Ol5Zih0"
      },
      "source": [
        "Note: We do not use the output indicators in this case as it will negate the chain of thought to instead enforce the formatting. It is important to explicitly incorporate the thought process desired in the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "1PbJihjtZih0"
      },
      "outputs": [],
      "source": [
        "def update_with_chain_of_thought_prompt(system_message: str, prompt_template: str):\n",
        "    chain_of_thought_instruction = \"Let's explicitly think step by step. My thought process is:\\n\"\n",
        "    chain_of_thought_system_format = \"Format: You must explicitly define the thought process and knowledge from the context to come to your conclusion for the question.\"\n",
        "    return system_message + \"\\n\" + chain_of_thought_system_format, prompt_template + \"\\n\" + chain_of_thought_instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "mnxHxmGXZih0"
      },
      "outputs": [],
      "source": [
        "chain_of_thought_system_message, chain_of_thought_prompt = update_with_chain_of_thought_prompt(system_message, prompt_template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luPy9J6zZih0",
        "outputId": "b13c22c4-ba52-4755-d343-83c22a0efc20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/imvenkata-ubs/beginner-llm-prompting-course/r/call/01921390-55de-76c2-89a8-d16fb632bf5f\n"
          ]
        }
      ],
      "source": [
        "chain_of_thought_response = llm_app(\n",
        "    system_message=chain_of_thought_system_message,\n",
        "    prompt_template=chain_of_thought_prompt,\n",
        "    context=context,\n",
        "    question=question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "awifAHLKZih0",
        "outputId": "d4e1f9fb-304e-4aad-dcb7-729b5387a29d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "### Zero-Shot Prompting **Explanation:** Zero-shot prompting involves presenting a task to a\n",
              "language model without providing any previous examples or context. The model uses its pre-trained\n",
              "knowledge to generate a response based solely on the input prompt.  **Example:** Imagine you want to\n",
              "know the sentiment of a movie review but you don't provide any prior examples of sentiment analysis.\n",
              "You simply ask: ``` Text: \"The movie was breathtaking and beautifully executed.\" Sentiment: ``` The\n",
              "model must infer the sentiment based solely on its pre-trained understanding of the language used in\n",
              "the review.  ### Few-Shot Prompting **Explanation:** Few-shot prompting gives the model a small\n",
              "number of examples (demonstrations) before presenting the actual task. This helps the model\n",
              "understand the specific task requirements and adjust its responses accordingly.  **Example:** You\n",
              "want to perform sentiment analysis again, but this time you provide a few examples first: ``` Text:\n",
              "\"This film was a fantastic journey through imagination.\" Sentiment: Positive Text: \"It was a dull\n",
              "and uninteresting plot that dragged on.\" Sentiment: Negative Text: \"The characters were\n",
              "underdeveloped and forgettable.\" Sentiment: Negative Text: \"The movie was breathtaking and\n",
              "beautifully executed.\" Sentiment: ``` Here, the model uses the provided examples to better gauge how\n",
              "to analyze the sentiment of the final review.  ### Chain-of-Thought (CoT) Prompting **Explanation:**\n",
              "Chain-of-Thought prompting involves instructing the model to generate a sequence of reasoning steps\n",
              "that lead to a final answer. This technique is particularly useful for complex reasoning tasks,\n",
              "where breaking down the problem into smaller, logical steps can help the model arrive at the correct\n",
              "conclusion.  **Example:** Suppose you ask a model to solve a math problem using Chain-of-Thought\n",
              "prompting: ``` Question: If a car travels 60 miles in 1.5 hours, what is its average speed? Chain of\n",
              "Thought: First, find the total distance traveled, which is 60 miles. Next, find the total time,\n",
              "which is 1.5 hours. To find the speed, divide the distance by the time. Speed = 60 miles / 1.5 hours\n",
              "= 40 miles per hour. Answer: 40 miles per hour. ``` In this example, the model explicitly outlines\n",
              "each step of its reasoning process before providing the final answer.  **Conclusion:** Each\n",
              "prompting technique serves different purposes and is suited to different types of tasks. Zero-shot\n",
              "is quick and uses no examples, few-shot provides context through examples, and Chain-of-Thought\n",
              "breaks down complex reasoning into manageable steps."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(chain_of_thought_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}